%\documentclass[a4paper]{article}
\documentclass{IEEEtran}

%\usepackage{times}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{nicefrac}

% Any macro definitions you would like to include
% These are not defined in the style file, because they don't begin
% with \bmva, so they might conflict with the user's own macros.
% The \bmvaOneDot macro adds a full stop unless there is one in the
% text already.
\def\eg{\emph{e.g.,}}
\def\ie{\emph{i.e.,}}
\def\etal{\emph{et al.}}
\def\vs{\emph{vs.}}

% macros for referencing figures, tables, equations and sections
\newcommand{\fref}[1]{Figure~\ref{#1}}
\newcommand{\eref}[1]{(\ref{#1})}
\newcommand{\tref}[1]{Table~\ref{#1}}
\newcommand{\sref}[1]{Section~\ref{#1}}
\newcommand{\aref}[1]{Algorithm~\ref{#1}}
\newcommand{\emptybox}[2]{\framebox[#1][l]{\rule[#2]{0pt}{0pt}}}

% maths macros
\def\G{G}
\def\Gx{G_x}
\def\Gy{G_y}
\def\Gxx{G_{xx}}
\def\Gxxs{G_{xx}(\sigma)}
\def\Gxy{G_{xy}}
\def\Gxys{G_{xy}(\sigma)}
\def\Gyx{G_{yx}}
\def\Gyy{G_{yy}}
\def\Gyys{G_{yy}(\sigma)}
\def\Ix{I_x}
\def\Iy{I_y}
\def\Ixsqr{I_{x^2}}
\def\Iysqr{I_{y^2}}
\def\Ixx{I_{xx}}
\def\Ixxs{I_{xx}(\sigma)}
\def\Ixy{I_{xy}}
\def\Ixys{I_{xx}(\sigma)}
\def\Iyy{I_{yy}}
\def\Iyys{I_{yy}(\sigma)}
\def\dtcwt{DT-$\mathbb{C}$WT}

\DeclareMathOperator*{\argmax}{arg\,max}

\def\deg{\ensuremath{^\circ}}
\def\rad{\ensuremath{\text{radians}}}
\def\by{\ensuremath{\times}}

% lengths for image sizes
\newlength{\qtrcol}\setlength{\qtrcol}{0.24\columnwidth}
\newlength{\halfcol}\setlength{\halfcol}{0.48\columnwidth}

% command for adding inline comment to text
\newcommand{\comment}[1]{}

% define title here so headers are updated, too
\def\ttl{Analysing Curvilinear Structures in Images}
\title{\ttl}
\author{Authors}

% define path to figures
\def\figroot{./figs}
\def\figpath{\figroot}


%-------------------------------------------------------------------------
% Document starts here
\begin{document}

\tableofcontents\clearpage

\maketitle

\begin{abstract}
Estimating orientation of image structure underpins applications including digital mammography, retinography and fingerprint analysis. We consider different choices of filter bank including those based on first and second derivatives, efficient Haar-like features and the Dual Tree Complex Wavelet Transform. We then investigate how standard regressors (linear regression, Boosting and Random Forests) may be adapted to use the responses to these filter banks in order to predict orientation of image structure. For a quantitative evaluation, we use synthetic images based on mammograms and the publicly available DRIVE database of retinal images, and show that Random Forests and the wavelet transform offer superior accuracy though at a cost in efficiency. Qualitative results are also presented for real mammograms and fingerprint images.
\end{abstract}

\input{citations.tex}


\section{Introduction}
% State the problem, and its impact on all stakeholders (those directly affected, and society at large e.g. the social and economic impact of treating the disease)
A curvilinear structure in an image appears as a ribbon or bar of finite width that is distinguishable from the surrounding structure, with a cross-sectional profile that is repeated along a linear, though not necessarily straight, path (\fref{f:line_examples}).

%What are their general characteristics?
%Defined locally by their cross-sectional profile and orientation, and assumed to extend in at least one direction normal to the profile (as opposed to a blob).

Detecting and measuring the properties of curvilinear structures in images is useful for many reasons~\cite{Ayres_Rangayyan_JEI07}:

\begin{itemize}
\item detecting distinctive patterns of vessels and fibrous tissue can improve quality of life and reduce costs associated with treating diseases such as retinopathy (\sref{s:retinopathy}) and breast cancer (\sref{s:mammography}) in advanced stages by aiding early diagnosis and treatment; %
\item spotting cracks and other similar defects in manufactured items such as roads, eggs can reduce costs associated with waste; %
\item biometrics based on ridge patterns in fingerprints~\cite{}, or the veins of the finger~\cite{} or hand~\cite{}, can bring criminals to justice and prevent further crime, or enhance the usability of technology by controlling access to sensitive data; %
\item detecting roads, railways and rivers in aerial photography can help to build maps automatically for applications such as providing relief in remote areas following a natural disaster.
\end{itemize}


\subsection{Aims and Objectives}

\begin{figure}[t]
\centering
\begin{tabular}{@{}c c c@{}}
\includegraphics[width=0.3\columnwidth]{\figpath/retina/02_test} &
\includegraphics[width=0.3\columnwidth]{\figpath/retina/02_segmentation_gabor_inv.png} &
\includegraphics[width=0.3\columnwidth]{\figpath/retina/002_orientation_masked} \\
%\includegraphics[height=0.15\textheight]{\figpath/retina/002_abs_error} \\
(a) & (b) & (c) \\
\noalign{\smallskip}
\end{tabular}
%
\caption{Estimating orientation in retinography: %
(a) input image; %
(b) segmentation of vessels by random forest classification of Gabor features; %
(c) orientation (indicated by colour) estimated using random regression over Gabor~features. The mask was not used to estimate orientation. %
%(c) magnitude of error (note the regions of high error at points of bifurcation.)
}
\label{f:retinography}
\end{figure}

% Specific aims of the study
Given an image, our aim is to determine where any linear structures exist in the image, and to measure values that correspond to low-level properties such as orientation, width, and cross-sectional profile. Although these properties form the basis of higher level, application-specific analysis (classifying structure as \emph{road}, \emph{rail} or \emph{river} in aerial photographs, for example), our focus is purely on computing the local attributes as accurately and robustly as possible; by improving the accuracy of the values we estimate, it follows logically that performance should improve for all higher level analysis methods that use these values as inputs.\footnote{Any algorithm that does not benefit from better input should be regarded with suspicion.}

%For example, any method to move from a map of vessel probabilities and predicted orientations in a retinograms, to an explicit grouping of pixels that belong to individual vessels, is likely to benefit from a priori knowledge of the spatial arrangement vessels in that image class and the physical model of how vessels grow and bifurcate. Such a method will therefore be very different from that needed to group a similar set of local information into the road, rivers etc present in an image for aerial analysis

With this focus, we have two objectives: extract, at every image location, structural information that is rich enough to capture the underlying image properties yet sparse enough to be computed efficiently; and combine this raw local information to predict output values of interest (such as orientation).


% Review other people's attempts at solving the problem, and why they are found wanting
\subsection{Related Work}
\input{related_work}

% Describe what we do, and why it is better than preceding works
\subsection{Our Contributions}
\input{our_contributions}




\clearpage
\section{Input Image Features}
\subsection{Filtering}
\label{s:filtering}
In this section we consider the theoretical requirements of a suitable filter bank, keeping foremost in our mind the application in which the responses will be used. That is the set of responses for any given structure pixel should be distinguishable from that of any background pixel; the filters should be directionally selective to predict orientation; likewise to predict structure width the responses should be selective across scale.

\begin{figure}[t]
\centering
\begin{tabular}{@{}c c c c@{}} % @{} removes padding around the edge of the table
\includegraphics[width=0.2\columnwidth]{figs/filtering/Gx} &
\includegraphics[width=0.2\columnwidth]{figs/filtering/Gxx} &
\includegraphics[width=0.2\columnwidth]{figs/filtering/Gxy} &
\includegraphics[width=0.2\columnwidth]{figs/filtering/Gxx-Gyy} \\
(a) & (b) & (c) & (d) \\
\noalign{\smallskip}
%
\includegraphics[width=0.2\columnwidth]{figs/filtering/mono_b} &
\includegraphics[width=0.2\columnwidth]{figs/filtering/mono_hx} &
\includegraphics[width=0.2\columnwidth]{figs/filtering/dt_cwt_r4} &
\includegraphics[width=0.2\columnwidth]{figs/filtering/dt_cwt_c4} \\
(e) & (f) & (g) & (h) \\
\noalign{\smallskip}
\end{tabular}
%
\caption{(a)~First derivatives $\Gx = \Gy^T$; (b-d)~Second derivatives, $\Gxx = \Gyy^T$, $\Gxy$; and $\Gxx-\Gyy$; (e,f)~Monogenic signal filters $B$ and $h_x = h_y^T$; (g,h)~Real and complex responses of the \dtcwt~ $15^\circ$ subband.}
\label{f:filters}
\end{figure}

\subsubsection{Gaussian derivatives}
\label{s:filtering_secondderivs}
%
We start by considering probably the most commonly used set of filters for linear structure detection (certainly within the context of vessel segmentation in medical images): derivatives of a Gaussian kernel.

Gaussian first derivatives - edge detection. Explain second derivatives.
%
The directional second-order derivative of a Gaussian generates an even (\ie~symmetric) image filter that resembles a bar or ridge feature. Like their first-order counterparts, second-order derivatives are steerable though they require three rather than two basis filters. The second-order basis filters are not separable, though a different formulation generates three equivalent basis filters -- $\Gxx$, $\Gyy$ and $\Gxy$ (\fref{f:filters_secondderivs}) -- that are. The response to a second-order derivative is given by
%
\begin{equation}
R(\theta) = \Ixx \cos^2(\theta) + \Iyy \sin^2(\theta) + \Ixy \sin(2\theta)
\label{e:secondderivs_response}
\end{equation}
%
\noindent where $\Ixx = \Gxx\ast I$, $\Iyy = \Gyy\ast I$ and $\Ixy = \Gxy\ast I$ are the responses to the three separable filters. This response function has four stationary points in the range $[0,2\pi)$, occuring at
%
\begin{equation}
\theta = \frac{1}{2} \tan^{-1}\left( \frac{2\Ixy}{\Ixx-\Iyy} \right).
\label{e:secondderivs_orientation}
\end{equation}
%
\noindent Two of these points, separated by $\pi\,\rad$ because the filter is rotationally symmetric, correspond to the directions in which the filter is aligned with the feature and the \emph{absolute} value of the response is maximal; the other two points correspond to the two perpendicular directions.

Because, however, the maximal absolute response may correspond to either a maximum or minimum (depending on whether the underlying feature is light-on-dark or vice versa) the only way to find out which of the two perpendicular directions has maximal absolute value is to evaluate the response at both and choose the direction corresponding to the larger absolute value. As a result, estimating orientation from second-order derivatives becomes a nonlinear problem.

Efficient approximations to computing second-order derivatives of the Gaussian can be achieved through Haar-like approximations to the derivative filters~\cite{Bay_etal_CVIU08} or by approximating the Gaussian filter and applying local finite differences~\cite{Kovesi_DICTA10}.

Note this approach is often reformulated by creating a Hessian matrix with the xx and yy derivates on the lead diagonal and the xy derivatives on the opposing diagonal. Solving this matrix produces eigen vectors with direction theta and thetaN and responses R.

G2D's work on the assumption that when steered to match the orientation of a structure, the response will be large, whilst the response in the perpendicular direction will be near-zero. Thus structures can be distinguished from flat backgrounds (both responses near-zero) or circular blob-like structures (both responses large). Equation () (or its Hessian reformulation) provides an elegant solution for determining this orientation analytically, from which the responses can be used to detect structure directly []. Alternatively the filters may be steered to multiple orientations over multiple scales and used as features in a machine learning algorithm [].

However, there are two problems in using second derivatives alone. Firstly, a strong edge in the image will produce an "echo" response that cannot be distinguished from the response at the centre of a CLS. This may seem an arbitrary construct; however it is easy find examples in real data. For example, the edge of the optic disc in a retinogram (Fig X) is often misclassified as a vessel, whilst similar artefacts may occur at the edges of lesions or near the pectoral muscle in mammograms.

Secondly, due to noise in the image, the symmetric profile of a CLS may be disrupted to the extent that not only does the equation [] produce inaccurate results, the responses themselves hold no usual information that a machine learning algorithm to take advantage of. Again, this can be seen clearly in real data, particularly in structures that have a width of only one or two pixels, such as the smallest vessels in retinograms.

The first problem tells us that it is not enough to only have filters designed to match the shape profile of the CLS we want to detect if such filters cannot distinguish other structures in the image background. The second problem shows we cannot rely on the assumptions we make about the CLS in real images. Combining both ideas motivates us to choose a filter bank that more generally represents an image. Our goal then is not to make a priori assumptions about how we want filters to respond to particular structures, but simply to ensure the set of filter responses produce a unique signature for differing structures. It is then up to our chosen machine learning algorithm to match the various signatures present in the training data to the output measure of interest.

Returning to Gaussian derivatives, we note the cause of both problems is that at a given scale and orientation, we can only compute the response to a filter with even symmetry. An intuitive solution is to supplement these filters with Gaussian 1st derivatives (as used most commonly in edge detection e.g. Canny) as used to heuristically discard edge echo responses in []. However, we prefer the solution recommended in [], using the Hilbert transform of the second derivatives. A steerable response can be computed from four separable basis filters, defined below:

The advantage of using Rh rover first derivatives is that it allows us to represent the responses as magnitude and phase:

We show in section X how this improves results, particularly for orientation (and width?) prediction.

\subsubsection{Gabor derivatives}
\label{s:filtering_gabor}
Next we consider Gabor filters.

A Gabor filter pair consists of one sine and one cosine function, in 2 dimensions and windowed with a Gaussian function. They are therefore directionally sensitive, and can also recover phase information from the underlying image region.

Because Gabor filters recover both orientation and phase information, they are a popular choice of filter in image processing applications~\cite{Daugman_TASSP88}. In general, however, they are neither separable nor steerable, although methods to approximate steerability have been explored~\cite{Teo_1987,Perona_PAMI95}. Therefore, they must be applied exhaustively over a discrete set of orientations which makes them expensive in terms of computation and (if all responses are to be stored simultaneously) memory.

%
\begin{equation}
g_{re}(x,y;\lambda,\theta,\sigma,\gamma) = \exp(-\frac{x'^2 + \gamma^2 y'^2}{2\sigma^2}\cos(2\pi \frac{x'}{\lambda})
\label{e:gabor_real}
\end{equation}
\begin{equation}
g_{im}(x,y;\lambda,\theta,\sigma,\gamma) = \exp(-\frac{x'^2 + \gamma^2 y'^2}{2\sigma^2}\sin(2\pi \frac{x'}{\lambda})
\label{e:gabor_imag}
\end{equation}
\begin{align}
x' = x\cos\theta + y\sin\theta \\
%
y' = -x\sin\theta + y\cos\theta
\label{e:gabor_xy}
\end{align}
%

As with Gaussian derivatives, these have been used extensively in detecting CLS, although again often only the even filter [eq X] is used as this is what is assumed will match the shape. For the same reason we match G2 with its Hilbert pair, we recommend using both odd and even parts, with maximum benefits obtained by combining them as a magnitude/phase pair and show experimentally the advantages of doing so in section X.

Note that unlike Gaussian derivatives, Gabor filters are neither separable nor steerable, making them much more expensive to compute. In contrast we now consider two further filtering schemes designed to measure magnitude and phase across and orientation and scale more efficiently: the monogenic signal; Gabor wavelets~\cite{Daugman_TASSP88}; and the Dual Tree Complex Wavelet Transform (\dtcwt{}~\cite{Kingsbury_PTRSLA99}), so far unexploited in applications analysing curvilinear structure.


\subsubsection{The Monogenic Signal}
\label{s:filtering_monogenic}
The monogenic signal~\cite{Felsberg_Sommer_TSP01} computes phase and orientation at a given image location using three filters: one even band-pass filter $B$, and an odd quadrature pair of filters $h_x(x,y) = x/f(x,y)$ and $h_y(x,y) = y/f(x,y)$ where $f(x,y) = 2\pi(x^2 + y^2)^{\frac{3}{2}}$ (\fref{f:filters_monogenic}). These filters are combined to compute local amplitude~($A$), phase~($\psi$) and orientation~($\theta$) at every location in the image:

\begin{align}
A       &= \sqrt{{I_B}^2 + {I_{hx}}^2 + {I_{hy}}^2}
\label{e:monogenic_amplitude} \\
%
\psi	  &= \tan^{-1}\left[ \frac{I_B}{\sqrt{{I_{hx}}^2 + {I_{hy}}^2}} \right]
\label{e:monogenic_phase} \\
%
\theta  &= \tan^{-1}\left[ \frac{I_{hy}}{I_{hx}} \right]
\label{e:monogenic_orientation}
\end{align}

\noindent where $I_B = I \ast B$, $I_{hx} = h_x \ast I_B$ and $I_{hy} = h_y \ast I_B$.

The local amplitude provides a magnitude of response that is consistent for all structures and orientations while the local phase provides a measure of symmetry (in a profile of the structure perpendicular to its orientation), varying from $-\pi/2$ for a negative line (\ie~a dark line on a light background), through $0$ for an edge, to $\pi/2$ for a positive line.

Though the monogenic signal combines both odd and even filters, the even filter $B$ is isotropic and therefore has no directional sensitivity. All orientation information therefore comes from the odd filters $h_x$ and $h_y$, such that orientation is not recovered for symmetric image features (\eg~at the centre of a bar or ridge).

\subsubsection{The Dual-tree Complex Wavelet Transform}
\label{s:filtering_dtcwt}

The Dual-Tree Complex Wavelet Transform (\dtcwt{}~\cite{Kingsbury_ACHA01}) is a directionally selective representation that combines the strengths of approximately shift-invariant coefficient magnitudes and local phase information with the computational efficiency of decimation (\ie~downsampling the image rather than increasing the filter size).

For a given pixel at a given scale, the \dtcwt{} combines the responses to a pairs of wavelets -- one real, one complex, and differing in phase by $90\deg$~(\fref{f:filters_dtcwt}) -- at six orientations: $\pm 15\deg$, $\pm 45\deg$ and $\pm 75\deg$. Because these filters are not exactly rotationally symmetric, the wave frequencies of the $\pm 45\deg$ sub-bands must be reduced so that they lie closer to those at $\pm 15\deg$ and $\pm 75\deg$, and all six sub-bands are adjusted so that the phase at the centre of the impulse response of each wavelet is zero~\cite{Kingsbury_ECSP06}.

% Multiresolution filtering
To compute filter responses at different scales, the image is repeatedly downsampled by a factor of two in every axis before applying the same six filter pairs again. To get the response for every scale at a given location on the original pixel grid, filter responses on a coarse grid at lower levels of the tree are interpolated with a bandpass method~\cite{Anderson_etal_ICIP05}.

% Advantages
The \dtcwt{} has the benefit of a low redundancy of just 4:1, making it feasible to store the decomposition of even large images. It is also efficient, relative to other methods such as Gabor filtering, through its use of downsampling.

% Disadvantages
Decimation does, however, introduce complications because the filters are not necessarily computed centrally over structures of interest. The phase of \dtcwt{} coefficients within the support of a structure therefore encodes both the symmetry of the structure and a spatial offset. By computing phase differences both spatially and across scale, however, it is possible both to recover local phase that is globally consistent for structure symmetry (and analogous to the phase returned from the monogenic signal) and to compute local orientation analytically~\cite{Anderson_ICIAR05,Anderson_SSP05}.

It is not clear, however, how to use responses to the \dtcwt{} filters to compute a single measure of curvilinear structure probability. Though we could select the maximum of the six oriented sub-band coefficients at each scale and combine them in a measure of phase congruency (as in a method based on the monogenic signal~\cite{Wai_etal_MICCAI04}), this would discard potentially useful information.

We therefore construct a feature vector that characterises each pixel by sampling \dtcwt{} coefficients from the six oriented sub-bands in each of the $s$ finest decomposition scales from a neighbourhood centred on the pixel, and transforming every complex response, $c$, to polar coordinates (\ie~magnitude, $|c|$, and angle, $\angle c$). Since orientation is only defined up to a rotation of $180^\circ$, however, the sign of the angle is arbitrary and so we use its absolute value, $|\angle c|$.

\subsubsection{Other stuff}
\label{s:filtering_extras}
Finally we acknowledge that there are of course many further filter banks we do not test in this paper, and for which an exhaustive comparison of results is unfeasible. However, we show that it is the properties we choose to implement for a given a filter bank (e.g. a magnitude/phase versus just an even/odd response, decimation versus increasing filter size, oversampling scales etc.) rather than the inherent properties of the filters that have the biggest effect in performance. In turn, given a set computational cost, this allows us to make an informed choice of suitable filter bank for any given data.

We also show that a filter bank selected given these general criteria can produce features that outperform features handcrafted for a particular application.

One interesting concept we do not test is the idea of learning an optimal set of arbitrary filters for a given set of images, as in []. However, we believe that such an approach is only beneficial if the filters are optimised with respect to the task they need to perform (in our case and in [], separating the responses for CLS and background pixels within a classifier) and cannot see how optimising with respect to some other task (such as reconstructing the image in a maximally sparse way) is intrinsically a desirable thing to do. That said a comparison with the results in [] would be desirable if quantitative results on the DRIVE and STARE datasets were made available.

\subsection{Composing feature vectors from filter responses}
\label{s:composing_features}
In this section we consider how, for any pixel, to combine the responses of a given filter bank into a feature vector. In the simplest form, we just concatenate the responses from the raw filters at all scales and orientations, however we also consider the following:

\subsubsection{Steering}
\label{s:composing_features_steering}
For the Gaussian derivatives (and their Hilbert transform), we can choose to steer the raw responses at each scale to a fixed set of directions spread evenly across the circle (e.g. in the same directions we apply the Gabor filterbank). This potentially produces features that can be more easily matched to the appropriate output measure and for experimentation purposes provides a more direct comparison to the Gabor and \dtcwt{} filter banks.

\subsubsection{Complex form}
\label{s:composing_features_complex}
As discussed in the previous section, for the Gaussian, Gabor and \dtcwt{} filter banks at a given scale and orientation we have a pair of responses that can be thought of as a complex number, where by custom we use the response to the even filter for the real part and the odd response for its imaginary counterpart. We can then choose either to include the real and imaginary parts as separate dimensions in the feature vectors or represent the pair of responses as magnitude and phase. The latter is arguably a more intuitive way to think of the responses - considering the image profile sampled at a given orientation and scale, the magnitude signifies if a feature is present in this 1D signal, whilst the phase tells us about the shape of the feature as it varies from a valley, to a step, to a ridge. Note also that if an image is rotated through 180 degrees, a complex response C = a+ib becomes C*=a-ib. Thus if we want to make our features responses ambivalent to 180 degree rotations we can use the absolute value of the imaginary part when computing phase.

\subsubsection{Rotation invariance}
\label{s:composing_features_rotation}
Given a set of responses at orientations spread evenly over the circle, a common approach is to select the orientation with maximal response and circular shift the remaining responses in the feature vector such that the maximal orientation for each feature vector occupies the same dimension. The intention is to produce feature vectors with rotational invariance thus collapsing the size of the feature space (proportional to the number of discrete orientations in the filter bank) and making it easier for the classifier to do its job. Whilst theoretically appealing we show in section X that this has no discernible benefit in practice. Note also, with responses over multiple scales, we can choose either to allow the responses in each scale to shift independently or choose a single maximum orientation (e.g. from the scale that produces maximum response) to circularly shift all scales. Here we take the former approach although we have experimented with both and found little difference in performance.

\subsubsection{Pooling neighbourhood responses}
\label{s:composing_features_neighbours}
In contrast to faffing with rotational invariance, a simple yet effective measure is to pool the responses from neighbouring pixels. Again we have experimented with more exotic sampling schemes in which we interpolate responses in a circular pattern about the pixel of interest, but have found that in practice simply sampling a 3x3 window of responses provides most benefit. Of course this increases the dimension of the feature vectors nine-fold, but with machine learning algorithms (such as random forests) designed to cope with large dimensional features and plenty of training data (which is nearly always the case in this set up given each pixel in an image is a sample and thus even a small set of images typically contain millions of samples) this needn't be a problem.

Flow chart…


\clearpage
\section{Target Output Labels}
Our approach to analyzing and understanding images containing linear structure is to use statistical learning methods to recognize patterns present in training data in order to predict useful information in previously unseen images. More specifically, we focus on three tasks: detecting linear structures in the image; discriminating between linear structures of different types (\eg~classifying ducts from spicules in mammograms); and measuring the orientation of linear structures. Though the input image features are common across all three tasks, the output label we wish to predict is task-dependent.

\subsection{Detecting Linear Structure}
\input{why/detect_lines/in_general}
\input{output_labels/label_detection}%

\subsection{Classifying Linear Structure}
\input{why/classify_lines/in_general}%
\input{why/classify_lines/in_mammograms}%
\input{output_labels/label_classification}%

\subsection{Measuring Orientation}
\label{s:measuring_orientation}
\input{why/measure_orientation/in_general}%
To address this problem, we assume that orientation can be expressed as a (typically nonlinear) function of the responses to a given set of filters. Theoretically, we know this to be true for some filter banks (\eg~second derivatives of a Gaussian), though there are some complications.

First, when filters are applied at more than one scale we must ensure that we use the responses from the best scale for the true line width; analytic methods~\cite{Karssemeijer_teBrake_TMI96,Mei_etal_IVC09} assume this is the scale with the greatest response, though this is not guaranteed in the presence of noise. Second, any analytic method that assumes noise to be additive and Gaussian may suffer when this is not the case; this is a particularly a risk in medical applications (\eg~ultrasound has multiplicative Rayleigh noise). Third, for some filter banks (such as the \dtcwt{}) an analytic solution is not available at all or is fiendishly complex at best.

\input{output_labels/label_orientation}


\clearpage
\section{Statistical Learning Methods}
Given a set of filter-bank outputs from different scales, the second step in estimating orientation is to combine them in some way. There are two basic approaches: to find the scale at which the total magnitude of response is greatest, and combine the different filter responses at that scale analytically~\cite{Karssemeijer_teBrake_TMI96,Mei_etal_IVC09}; or to use a regression learning approach to combine the filter responses across all scales and orientations~\cite{Berks_etal_IPMI11}.

In this work, we consider three classifiers of varying complexity: a linear classifier; a boosted classifier; and a Random Forest.

\subsection{Linear Classification}
\label{s:learning_linear}
\input{methods/machine_learning/linear_regression/regression_linear.tex}

%\subsection{Logistic Classification}
%\label{s:learning_logistic}
%\input{regression_logistic}

\subsection{Boosted Learning}
\label{s:learning_boosted}
\input{methods/machine_learning/boosting/regression_boosted.tex}%

\subsection{Random Forests}
\label{s:learning_forest}
\input{methods/machine_learning/decision_trees/rf_tree.tex}
\input{methods/machine_learning/random_forests/rf_background.tex}
\input{methods/machine_learning/random_forests/rf_detection.tex}
\input{methods/machine_learning/random_forests/rf_orientation.tex}%

Considering that curvilinear structure has a well-defined orientation, the confidence in an orientation estimate can also be used as a substitute for detection.

\subsection{Sampling Data}
\label{s:learning_sampling_data}
The final step in our method is to determine how we sample data for the forests. We have two schemes, one for running experiments on training data (e.g. to evaluate parameter options), the other for making final predictions on test data.
In the first scheme, we simply take some fixed size random subsample of pixels across the whole training data, with an equal number of background and foreground pixels (although only the foreground pixels are used orientation and width prediction). We then take a bootstrap sample of this data to train each tree in the forest. To test the forest, we take a second subsample from the pixels in the main training data not used in the first set. We can repeat this scheme, taking different random subsamples at every iteration, to compute a measure of uncertainty in prediction performance.
To make final predictions on the test data, we adopt a slightly more complicated sampling scheme that aims to better use all the data in the training set. In the first stage, we sample a different random subset of the training data for each tree during forest building, recording which pixels were selected. We then use the forest to predict all the training data, where at each pixel we aggregate only those predictions from trees for which the pixel wasn't selected. We can thus produce an unbiased prediction error at each pixel, analogous to the out-of-bag error described in Breiman's original random forest work [].
We then build a second forest, where again we sample a different random subset of the training data for each tree, only this time rather than uniformly sampling from the data, we weight the samples according to equation X,
%
\begin{equation}
Er = x
\label{e:reweight_sampling}
\end{equation}
%
where Ep is the prediction error described above and is defined separately for detection, orientation and width prediction as follows:
%
\begin{align}
E   &= a
\label{e:reweight_detect} \\
%
E	&= b
\label{e:reweight_orientation} \\
%
E   &= c
\label{e:reweight_width}
\end{align}
%
This has the effect of oversampling pixels that were poorly predicted in the first forest (with lambda controlling the level of oversampling versus uniform sampling) and results in a significant improvement in overall prediction performance. Note that the second stage of this process can be repeated to determine a suitable value for lambda. Indeed subject to time constraints, we could iterate until our predictions in the training data converge. In practice however, we evaluate performance for a fixed set of values for lambda and select the best.
The resulting forest can then be used to make predictions for all images in the test data.


\clearpage
\section{Data \& Applications}
We evaluate our methods on three real datasets, two of which containing retinograms and the third containing images of the?. These data are described below.
%
\subsection{DRIVE}
\label{s:dataset_drive}
%
\input{what_is/diabetic_retinopathy}

\input{why/detect_lines/in_retinograms}

\input{why/measure_orientation/in_retinograms}
%

The publicly available DRIVE dataset~\cite{Staal_etal_TMI04} contains 40 full colour, JPEG compressed retinogram images (\fref{f:retinography}a) that originate from a diabetic neuropathy screening program in The Netherlands, where subjects were aged 25-90. The images were acquired using a Canon CR5 non-mydriatic 3CCD camera with a 45 degree field of view (FOV) and 8 bits per colour plane, and are $768 \by 584$ pixels in size. The field of view is defined by a mask, provided with every image, that results in a cropped image $565 \by 584$ pixels in size.

Forty images from a total of 400 were selected for the dataset, seven of which exhibit signs of mild early diabetic retinopathy. These 40 are split into 20 training and 20 test images. Every image comes with at least one mask (test images have two), hand-labelled by human observers, that define ground truth vessel segmentations (\fref{f:fig_drive_examples}).

\subsection{STARE}
\label{s:dataset_stare}
STARE is basically the same as DRIVE. What more can I say?

\subsection{Fibre}
\label{s:dataset_stare}

\subsection{Synthetic images}
\label{s:dataset_synthetic}

In addition, we use synthetic data to explicate particular points discussed in section X. Each synthetic image is created as follows:
Generate a line of random direction, contrast and width, with elliptical profile centred in 64x64 background. The background may either be flat or containing an edge of random orientation and contrast. The image is then corrupted by signal dependent Rician noise, subject to equation X.
%
\begin{equation}
I_{noise} = I
\label{e:rician_noise}
\end{equation}
%


\clearpage
\section{Experiments \& Results}
\label{s:experiments}

We first show use sets of synthetic data with increasing noise to highlight the benefit of learning to predict orientation as opposed to relying on analytical methods (we take it as given now that learning is established as superior to analytical methods for detecting structure), and to examine the benefit of using both odd and even filters.
We then test all combinations of composing feature vectors from our four filter banks using subsamples of the training data for each real dataset.
Finally, we use the best performing feature vector composition and construct forests to apply to the test data for the DRIVE (and STARE?) data as described in section X, and compare the results to previous work.

\subsection{Experiment 1: Synthetic data, increasing noise}

In our first experiments we use a series of synthetic datasets, to each of which we've added an increasing amount of noise. With these we show how the assumptions needed to make accurate predictions analytically are violated as noise increases. As a result, we show the advantage a learning approach brings as noise increases and the relevance of this result to real data.

For now, we limit our filters to the even (real) parts of both Gaussian 2nd derivatives and Gabor filters.

For the former we use the three separable filters $\Gxxs$, $\Gyys$ and $\Gxys$ described in equation X, applied across 5 scales, $\sigma \in S=\{1, 2, 4, 8, 16\}$. These filters produce image responses at pixel $p$, $\Ixxs$, $\Iyys$ and $\Ixys$ respectively, and using equation X, combine to produce a steered response $I_{\theta}(p | \sigma)$, which is maximised when $\theta = Fred$. For a given pixel, we can thus define the scale of maximum response as
%
\begin{equation}
\hat{\sigma}_G(p) = \argmax_{\sigma} |I_{\theta}(p | \sigma)|
\label{e:max_scale_gauss}
\end{equation}
%
and label the associated orientation and response $\hat{\theta}_G(p)$, $\hat{I}_{G}(p)$.

If we now consider the synthetic data, described in section X. Recall that each pixel in an image can either be labelled as belonging to the foreground (\ie lying on a CLS) or background, and that each foreground pixel $p$ is associated with a known orientation $\phi(p)$ and width $w(p)$. If we sample a set of foreground and background pixels from each synthetic dataset and label these $\mathfrak{F}(\lambda)$ and $\mathfrak{B}(\lambda)$ respectively, where $\lambda$ signifies the level of noise added to the data, we can then analyse the following:

%whilst for the Gabor filters we use ...? In both cases, we apply the filters over 5 scales, starting with a finest scale of $\sigma = 1$ and increasing in octaves up to $\sigma = 16$.
\begin{itemize}

    \item For $p \in \mathfrak{F}(\lambda)$, how often does $\hat{\sigma}_G(p) = \sigma$ as the real line width $w(p)$ varies? (\fref{f:synthetic_exp1}a-d)
    \item For $p \in \mathfrak{F}(\lambda)$, how accurate is $\hat{\theta}_G(p)$ as a predictor of $\phi(p)$? (\fref{f:synthetic_exp2}a)
    \item How do the distributions of $\hat{I}_{G}(p)$ compare for $p \in \mathfrak{F}(\lambda)$ and $p \in \mathfrak{B}(\lambda)$? (\fref{f:synthetic_exp1}k-n)
\end{itemize}

Similarly, for Gabor filters, we use the Real filters $Re(\sigma, \theta)$ defined in equation X, at 5 scales and 6 orientations ($ \sigma = S$, $\theta = \{ \nicefrac{i\pi}{6}\}$), producing image responses $I_{Re}(p | \sigma, \theta)$.

We can thus define
%
\begin{equation}
\hat{\sigma}_{Re}(p), \hat{\theta}_{Re}(p) = \argmax_{\sigma, \theta} |I_{Re}(p | \sigma, \theta)|
\end{equation}
%
with associated response $\hat{I}_{Re}(p)$, and subsequently analyse:

\begin{itemize}
    \item For $p \in \mathfrak{F}(\lambda)$, how often does $\hat{\sigma}_{Re}(p) = \sigma$ as the real line width $w(p)$ varies? (\fref{f:synthetic_exp1}f-i)
    \item For $p \in \mathfrak{F}(\lambda)$, how often does $\hat{\theta}_{Re}(p) = \theta$ as the $\phi(p)$ varies? (\fref{f:synthetic_exp1}u-x)
    \item How do the distributions of $\hat{I}_{Re}(p)$ compare for $p \in \mathfrak{F}(\lambda)$ and $p \in \mathfrak{B}(\lambda)$? (\fref{f:synthetic_exp1}p-s)
\end{itemize}

%
\begin{figure*}[t]
\centering
\begin{tabular}{@{}c c c c c@{}} % @{} removes padding around the edge of the table
\includegraphics[width=0.18\textwidth]{figs/synthetic/syn_lines_g2d_scales_0} &
\includegraphics[width=0.18\textwidth]{figs/synthetic/syn_lines_g2d_scales_1} &
\includegraphics[width=0.18\textwidth]{figs/synthetic/syn_lines_g2d_scales_2} &
\includegraphics[width=0.18\textwidth]{figs/synthetic/syn_lines_g2d_scales_3} &
\includegraphics[width=0.18\textwidth]{figs/retina/ret_vessels_g2d_scales} \\
(a) & (b) & (c) & (d) & (e)\\
\noalign{\smallskip}

\includegraphics[width=0.18\textwidth]{figs/synthetic/syn_lines_gabor_scales_0} &
\includegraphics[width=0.18\textwidth]{figs/synthetic/syn_lines_gabor_scales_1} &
\includegraphics[width=0.18\textwidth]{figs/synthetic/syn_lines_gabor_scales_2} &
\includegraphics[width=0.18\textwidth]{figs/synthetic/syn_lines_gabor_scales_3} &
\includegraphics[width=0.18\textwidth]{figs/retina/ret_vessels_gabor_scales} \\
(f) & (g) & (h) & (i) & (j)\\
\noalign{\smallskip}

\includegraphics[width=0.18\textwidth]{figs/synthetic/syn_lines_g2d_responses_cdf_0} &
\includegraphics[width=0.18\textwidth]{figs/synthetic/syn_lines_g2d_responses_cdf_1} &
\includegraphics[width=0.18\textwidth]{figs/synthetic/syn_lines_g2d_responses_cdf_2} &
\includegraphics[width=0.18\textwidth]{figs/synthetic/syn_lines_g2d_responses_cdf_3} &
\includegraphics[width=0.18\textwidth]{figs/retina/ret_vessels_g2d_responses_cdf}  \\
(k) & (l) & (m) & (n) & (o)\\
\noalign{\smallskip}

\includegraphics[width=0.18\textwidth]{figs/synthetic/syn_lines_gabor_responses_cdf_0} &
\includegraphics[width=0.18\textwidth]{figs/synthetic/syn_lines_gabor_responses_cdf_1} &
\includegraphics[width=0.18\textwidth]{figs/synthetic/syn_lines_gabor_responses_cdf_2} &
\includegraphics[width=0.18\textwidth]{figs/synthetic/syn_lines_gabor_responses_cdf_3} &
\includegraphics[width=0.18\textwidth]{figs/retina/ret_vessels_gabor_responses_cdf} \\
(p) & (q) & (r) & (s) & (t)\\
\noalign{\smallskip}

\includegraphics[width=0.18\textwidth]{figs/synthetic/syn_lines_gabor_ori_subbands_0} &
\includegraphics[width=0.18\textwidth]{figs/synthetic/syn_lines_gabor_ori_subbands_1} &
\includegraphics[width=0.18\textwidth]{figs/synthetic/syn_lines_gabor_ori_subbands_2} &
\includegraphics[width=0.18\textwidth]{figs/synthetic/syn_lines_gabor_ori_subbands_3} &
\includegraphics[width=0.18\textwidth]{figs/retina/ret_vessels_gabor_ori_subbands} \\
(u) & (v) & (w) & (x) & (y)\\
\noalign{\smallskip}

\end{tabular}
%
\caption{Need to complete caption}
\label{f:synthetic_exp1}
\end{figure*}
%

%
\begin{figure*}[t]
\centering
\begin{tabular}{@{}c c c c c@{}} % @{} removes padding around the edge of the table
\includegraphics[width=0.18\textwidth]{figs/synthetic/syn_lines_g2d_noise_cdf} &
\includegraphics[width=0.18\textwidth]{figs/synthetic/syn_lines_g2d_scales_cdf} &
\includegraphics[width=0.18\textwidth]{figs/synthetic/syn_lines_g2d_scales_v_width} &
\includegraphics[width=0.18\textwidth]{figs/retina/ret_vessels_g2d_scales_cdf} &
\includegraphics[width=0.18\textwidth]{figs/retina/ret_vessels_g2d_scales_v_width} \\
(a) & (b) & (c)& &\\
\noalign{\smallskip}
\includegraphics[width=0.18\textwidth]{figs/synthetic/syn_lines_g2d_RF_noise_cdf} &
\includegraphics[width=0.18\textwidth]{figs/synthetic/syn_lines_g2d_RF_scales_cdf} &
\includegraphics[width=0.18\textwidth]{figs/synthetic/syn_lines_g2d_RF_scales_v_width} &
\includegraphics[width=0.18\textwidth]{figs/retina/ret_vessels_g2d_RF_scales_cdf} &
\includegraphics[width=0.18\textwidth]{figs/retina/ret_vessels_g2d_RF_scales_v_width} \\
(d) & (e) & (f)& &\\
\noalign{\smallskip}
\end{tabular}
%
\caption{Need to complete caption}
\label{f:synthetic_exp2}
\end{figure*}
%
If we first consider the left most column of \fref{f:synthetic_exp1}, in which each of the above analyses is shown for the case where no noise is present in the synthetic images, we can see that both sets of filters behave idealistically: that is, *scale (both filters), and orientation (Gabor) sub-bands divide up properly*; the distributions of $\hat{I}_{Re}(p)$ and $\hat{I}_{Re}(p)$ divide perfectly for foreground and background pixels; and, as shown by the blue line in \fref{f:synthetic_exp2}a, $\hat{\theta}_G(p)$ is a near perfect predictor of $\phi(p)$, with over 95\% of pixels having a prediction error of less than $\pm5\deg$.

However, as noise is added to the images, the filters ideal behaviour degenerates. In \fref{f:synthetic_exp1} (a-e) and (f-i), we can see that the filter scale that produces maximum response is no longer a reliable indicator of line width. Likewise, for Gabor filters, the correct $\hat{\theta}_{Re}(p)$ (in the sense that $\hat{\theta}_{Re}(p)$ lies closest to $\phi(p)$), is selected for less than half the pixels, and worse still, the likelihood of the correct band being selected appears to vary with orientation, with lines near to the horizontal or vertical more likely to be produce a correct match. Considering the direct prediction of orientation using $\hat{\theta}_{G}(p)$, we can see in \fref{f:synthetic_exp2} that this reduces in accuracy, to the point where only half the pixels have a prediction error less than $\pm20\deg$. In terms of separating foreground and background pixels, we can see that the distributions of $\hat{I}_{G}$ and $\hat{I}_{Re}$ become significantly overlapped (\fref{f:synthetic_exp1}(k-n, p-s)).

The difficulty in selecting a filter scale based on maximum absolute response in noisy images, leads us to question whether, for analytical methods, applying multiple scales really is beneficial. This idea is explored further in \fref{f:synthetic_exp2}b, in which the error distributions of $E_{\hat{\theta}_{G}}$ are shown using each scale separately, along with the original multiscale method, for data with a noise level of $\lambda = 2$. We can see that the multiscale is clearly outperformed by at least two of the individual scales ($\sigma=4$ and $\sigma=8$). The problem with selected an individual scale, is that prediction accuracy is naturally biased towards line widths close to the selected filter scale. This is highlighted in \fref{f:synthetic_exp2}c, in which an estimate of the mean absolute prediction error is shown as a function of the true line width. As expected, the predictions using a filter with $\sigma=8$ are much more accurate for the widest lines, with a drop off in accuracy between $w(p) = 8$ and $w(p) = 1$ of over $13\deg$.

Of course, the specific relationship between the responses at different scales is a product of the non-spatially correlated noise model (which increases the random high-frequency components in the images, leading to disproportionately  high responses to the $\sigma = 1$ filters) coupled with lines that effectively have infinite length (thus biasing prediction accuracy in favour of large scale filters, regardless of line width, because they have the benefit of smoothing out noise over a larger area without the penalty of including neighbouring structures in their support region). However, to show the relevance of these experiments to real data, we repeat each of the above analyses with a set of foreground and background pixels randomly sampled from the retinogram DRIVE training database. The relevant plots are shown in the rightmost column of \fref{f:synthetic_exp2} and in \fref{f:synthetic_exp2} d and e. We see that the relationship between scales has changed, with the finest scale filter much lesser likely to produce maximum response and the bias towards large filters for prediction accuracy removed. However, the general problems associated with analytically selecting response and predicting orientation remain, and we again see that in terms of overall errors, we are better off using a single scale (although again, whichever scale we choose will have a performance biased towards lines with widths matched to that scale).

We conclude these initial experiments by building our first forests. Using the responses to the Gaussian second derivatives as features, and orientation as output we train forests using all scales, and each scale individually, for each noise level in the synthetic data (using samples from an independently generated set of training images) and for the DRIVE data (using a set of pixels from the training images sampled to have no overlap with the set tested), using the method described in section X. The resulting plots of prediction error are shown in the second row of figure \fref{f:synthetic_exp2}.

For the synthetic data, we see how the addition of noise causes a much smaller reduction in prediction accuracy than for the analytic method (figure \fref{f:synthetic_exp2}f). Indeed, even with a noise level of $\lambda = 3$, the prediction error at the 50th percentile is just $\pm5\deg$. Moreover, forests using all scales in the feature vector outperforms any of forests using a single scale (although for the reasons discussed above, using just filters with $\sigma=8$ perform very strongly in these data), suggesting that by regressing over the training data we have been better able to combine the information across all scales.

These trends are repeated for the real retinogram data (Figure \fref{f:synthetic_exp2}j-k), with the advantage of including all scales in the feature vectors even more clearly in evidence. Note in particular, how flat the estimation of mean errors in the regressed "All scales" predictions of Figure \fref{f:synthetic_exp2}k are as a function of vessel width compared to any of the individual scale, analytic predictions in Figure \fref{f:synthetic_exp2}e.

Thus we can conclude from these experiments that whilst analytic methods work as intended for images no or very little noise, once a realistic level of noise is include the assumptions required to accurately predict orientation or obtain a response that clearly separates foreground and background pixels are invalidated. In such situations, using the same filter responses as input to a random forest regressor will produce significantly more accurate predictions. Moreover, regressing allows us to successfully integrate information across multiple filter scales, thus providing a more consistent prediction for structures of varying width.

\subsection{Experiment 2: Synthetic data, Line vs Edge }
\label{s:experiments_2}
[may leave out or combine with 1]
%
\begin{itemize}
  \item Line detection when on edge: with/without odd component, 1x1 vs 3x3
  \item Performance specifically at edge:

  %
  \begin{itemize}
    \item Show figure on synthetic data
    \item Real example from DRIVE data
  \end{itemize}
  %
\end{itemize}
%
\begin{figure}[t]
\centering
\begin{tabular}{@{}c c c@{}}
\includegraphics[width=0.3\columnwidth]{\figpath/retina/02_optic} &
\includegraphics[width=0.3\columnwidth]{\figpath/retina/02_optic_g2d_inv} &
\includegraphics[width=0.3\columnwidth]{\figpath/retina/02_optic_g12d_inv} \\
%\includegraphics[height=0.15\textheight]{\figpath/retina/002_abs_error} \\
(a) & (b) & (c) \\
\noalign{\smallskip}
\end{tabular}
%
\caption{Detecting vessels in retinography: %
(a) Magnified region containing optic disk; %
(b) Segmentation using only even filter responses as features: the false positive predictions
at the edge of the disk have similar strength to neighbouring vessels; %
(c) Segmentation using odd and even filter responses: false positives are still present, but at a
much lower strength to nearby vessels;
}
\label{f:retinography}
\end{figure}

\subsection{Experiment 3: Comparing Filters banks and Feature Vector Compositions}
\label{s:experiments_3}

Having established a case for regressing orientation, and the advantages of learning methods in general, we now apply random forests both to the task of predicting the presence of lines and their associated orientation. In these experiments we seek to test all possible configurations discussed in section X.

As a result, for the Gaussian 2nd derivatives we now also apply its Hilbert transform, using the separable filters $H_1$, $H_2$, $H_3$ and $H_4$ (equations X). Similarly for Gabor filters, we use both the real and imaginary parts. To these, we add the \dtcwt and Monogenic Signal filters introduced in section X.

For data, we use the retinal DRIVE and fibre datasets described in section X\footnote{We have also applied these experiment to the retinal STARE database, however the results are so similar to those of the DRIVE data that have been omitted here. They are available...}. In each case, we use only the training images, from which we randomly sample 10 training and test sets, each of which contain 100,000 foreground and 100,000 background pixels. Again, we ensure there are no overlaps between any training set and the test set it is paired with (we do not, however, force exclusivity between the repeat sets).

For each combination of filters and datasets, we train a random forest to classify between foreground and background (thus performing detection) and to regress the orientation of the foreground pixels. Each forest is applied to the respective test sets, and the results recorded. For detection, we compute an ROC curve and summarise performance by the area under the curve ($A_z$), for orientation we use the median absolute error. The results for each test are presented in \tref{t:drive_training_c} and \tref{t:fibre_training_c}, giving the mean and standard deviation of each test statistic over the 10 repeats.

In addition to performance measures, we compute the time taken to generate a single feature vector for each filter combination. This comprises the time for filtering the image and the time for combining the raw filter responses into a feature vector, and is computed under the assumption that we are extracting feature vectors for all pixels in an image (which is near negligible for non-decimating filters, but forms a significant part of the total time in decimating schemes such as the \dtcwt where filtering is extremely fast but we need to interpolate the raw response). We also give the total number of dimensions in each vector, which itself will have an effect on the time taken for training and making predictions, and will also effect the total number of feature vectors that can be held in memory at any moment. *Should we discuss considerations of training set size, and if so, where*.

Below we summarise the main findings of these experiments.

Overall, feature vectors composed of Gabor or Gaussian filter outputs proved most successful at the detection task and were both significantly better than the \dtcwt, which in turn was significantly better than the Monogenic Signal. For orientation prediction, there was a clear and significant hierarchy of performance, in the order Gabor, \dtcwt, Gaussian then Monogenic.

For the Gaussian filters, steering the coefficients to generate responses equally spaced around the circle benefitted both tasks, with a greater effect for predicting orientation.

For all filter banks, including odd filters along with the even filters proved beneficial, and is particularly necessary for the \dtcwt (where neither filter is completely odd or even, thus neither is an ideal match for the majority of structures found in the images). Converting the odd and even responses into a magnitude/phase pair appears to make little difference to detection, but significantly benefits orientation. Similarly, "folding phase" (see secton x) to produce feature vectors invariant to $180\deg$ shifts produces a further improvement in performance for predicting orientation, whilst not effecting the results of detection significantly.

Attempting to make feature vectors rotationally invariant to smaller shifts in angle by circularly permuting responses about the oriented sub-band of maximal response also proved to be of no significant benefit to (steered) Gaussian and Gabor filters, whilst it significantly reduced performance for the \dtcwt.

In contrast, pooling neighbourhood responses proved beneficial in all cases, regardless of filter type and combination (note that for reasons of space we have shown $W=1$ and $W=3$ comparisons for just the initial combination of filters, and displayed results for $W=3$ thereafter. However all results are available...). Doing so provides most benefit where only one of an odd/even filter pair is used, particularly so for odd filters which without pooling responses are totally unsuited to the task.

We have previously observed that including multiple scales in the feature vectors is beneficial using just the even part of the Gaussian filters, and this result is confirmed here for all filter banks using magnitude phase pairs. Adding further scales in between the octave scales for Gaussian and Gabor filters, provides a small benefit for detection and a significant improvement in predicting orientation. Likewise, adding additional oriented sub-bands within each scale also help orientation prediction, although it appears adding additional scales is more beneficial. However, in both cases the size of the resulting feature vectors are too large to make them practically useful on a standard PC (we used a dedicated computational cluster with 16GB of memory to produce these results).

Similarly, including responses from all feature banks in a single feature vector improves upon the results of any of the filter banks individually, but at an impractical computational cost. Moreover, the improvement in performance is less than adding additional scales to either the Gaussian or Gabor filter banks.

In terms of comparing the two datasets, the trends observed between the different feature vector combinations were the same in both cases. Overall the performance was lower in fibre set. this is likely to be a reflection of both the harder nature of the task (noisier images, with less well-defined structures) and the fact that unlike in our comparison to other work described in the next section, we did not allow any tolerance in the spatial classification task.

*Do we also need to mention: effect of balancing foreground/background sets, number of pixels in training sets, number of trees in forest, stopping parameters in forest, etc? We have results for all of these...*

%
%
\begin{table*}[t]
\centering
\small
\input{experiments/tab_DRIVE_training_cost_sort.tex}
\caption{Detecting and predicting the orientation of retinal vessels. DRIVE database, training images:
...}
\label{t:drive_training_c}
\end{table*}
%
\begin{table*}[t]
\centering
\small
\input{experiments/tab_fibre_training_cost_sort.tex}
\caption{Detecting and predicting the orientation of nerve fibres in confocal corneal microscopy images. Training images:
...}
\label{t:fibre_training_c}
\end{table*}
%
\subsection{Experiment:4 - Bang, the real thing}
\label{s:experiments_4}
%
\begin{table}[h]
\centering
\small
\input{experiments/tab_DRIVE_test.tex}
\caption{Final detection and orientation prediction results for retinogram vessels.}
\label{t:ret_test}
\end{table}
%
\begin{table}[h]
\centering
\small
\input{experiments/tab_fibre_test.tex}
\caption{Final detection and orientation prediction results for fibres in CCM images.}
\label{t:fibre_test}
\end{table}
%
\begin{figure*}[t]
\centering
\begin{tabular}{@{}c c c c c@{}} % @{} removes padding around the edge of the table
\includegraphics[width=0.18\textwidth]{figs/retina/19_DRIVE_ret} &
\includegraphics[width=0.18\textwidth]{figs/retina/19_DRIVE_segmentation_gabor_inv} &
\includegraphics[width=0.18\textwidth]{figs/retina/19_DRIVE_segmentation_gh2da_inv} &
\includegraphics[width=0.18\textwidth]{figs/retina/19_DRIVE_segmentation_mono_inv} &
\includegraphics[width=0.18\textwidth]{figs/retina/19_DRIVE_segmentation_dt_inv} \\
(a) & (b) & (c) & (d) & (e) \\
\noalign{\smallskip}
%
\includegraphics[width=0.18\textwidth]{figs/retina/08_DRIVE_ret} &
\includegraphics[width=0.18\textwidth]{figs/retina/08_DRIVE_segmentation_gabor_inv} &
\includegraphics[width=0.18\textwidth]{figs/retina/08_DRIVE_segmentation_gh2da_inv} &
\includegraphics[width=0.18\textwidth]{figs/retina/08_DRIVE_segmentation_mono_inv} &
\includegraphics[width=0.18\textwidth]{figs/retina/08_DRIVE_segmentation_dt_inv} \\
(f) & (g) & (h) & (i)  & (j) \\
\noalign{\smallskip}
\end{tabular}
%
\caption{Detecting vessels in retinography: Best (a-e) and worst (f-j) results in the test set. %
(a,f) original image; %
(b,g) Gabor; %
(c,h) Gaussian; %
(d,i) Monogenic Signal; %
(e,j) \dtcwt; %
}
\label{f:drive_segmentations}
\end{figure*}
%
\begin{figure*}[t]
\centering
\begin{tabular}{@{}c c c c c@{}} % @{} removes padding around the edge of the table
\includegraphics[width=0.18\textwidth]{figs/fibre/03_fibre_ccm} &
\includegraphics[width=0.18\textwidth]{figs/fibre/03_fibre_segmentation_gabor_inv} &
\includegraphics[width=0.18\textwidth]{figs/fibre/03_fibre_segmentation_gh2da_inv} &
\includegraphics[width=0.18\textwidth]{figs/fibre/03_fibre_segmentation_mono_inv} &
\includegraphics[width=0.18\textwidth]{figs/fibre/03_fibre_segmentation_dt_inv} \\
(a) & (b) & (c) & (d) & (e) \\
\noalign{\smallskip}
%
\includegraphics[width=0.18\textwidth]{figs/fibre/51_fibre_ccm} &
\includegraphics[width=0.18\textwidth]{figs/fibre/51_fibre_segmentation_gabor_inv} &
\includegraphics[width=0.18\textwidth]{figs/fibre/51_fibre_segmentation_gh2da_inv} &
\includegraphics[width=0.18\textwidth]{figs/fibre/51_fibre_segmentation_mono_inv} &
\includegraphics[width=0.18\textwidth]{figs/fibre/51_fibre_segmentation_dt_inv} \\
(f) & (g) & (h) & (i)  & (j) \\
\noalign{\smallskip}
\end{tabular}
%
\caption{Detecting fibres in CCM images: Best (a-e) and worst (f-j) results in the test set. %
(a,f) original image; %
(b,g) Gabor; %
(c,h) Gaussian; %
(d,i) Monogenic Signal; %
(e,j) \dtcwt; %
}
\label{f:fibre_segmentations}
\end{figure*}
%
%
\begin{figure}[t]
\centering
\begin{tabular}{@{}c c@{}} % @{} removes padding around the edge of the table
\includegraphics[width=0.48\columnwidth]{figs/retina/34_DRIVE_ret} &
\includegraphics[width=0.48\columnwidth]{figs/retina/34_resampling_difference} \\
(a) & (b) \\
\noalign{\smallskip}
\end{tabular}
%
\caption{What effect does resampling have? %
(a) Retinogram; %
(b) Difference between prediction maps with and without resampling. Red indicates a reduced vessel probability with resampling, blue an increased vessel probability. Note the reduced prediction probability at the edge of vessels and for the false predictions caused by pathology in the image; %
}
\label{f:retinography}
\end{figure}
%
%
\begin{figure}[t]
\centering
%
\includegraphics[width=0.9\columnwidth]{figs/retina/DRIVE_test_detection_roc_zoom}
%
\caption{Detecting vessels in retinography: DRIVE test ROCs %
}
\label{f:retinography}
\end{figure}
%
%
\begin{figure}[t]
\centering
%
\includegraphics[width=0.9\columnwidth]{figs/retina/DRIVE_test_orientation_cdf}
%
\caption{Predicting vessel orientation in retinography: DRIVE database %
}
\label{f:retinography}
\end{figure}
%
\begin{figure}[t]
\centering
%
\includegraphics[width=0.9\columnwidth]{figs/retina/STARE_detection_roc_zoom}
%
\caption{Detecting vessels in retinography: STARE ROCs %
}
\label{f:retinography}
\end{figure}
%
\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{figs/retina/STARE_orientation_cdf}
%
\caption{Predicting vessel orientation in retinography: STARE database %
}
\label{f:retinography}
\end{figure}
%

\begin{itemize}
  \item DRIVE

  \begin{itemize}
    \item State of the art detection
    \item Orientation?
  \end{itemize}

  \item STARE
  \begin{itemize}
    \item Classify using DRIVE, still good performance
  \end{itemize}

  \item Fibre
  \begin{itemize}
    \item Thin and allow tolerance as per Dabbah paper
  \end{itemize}

\end{itemize}

\clearpage
\section{Discussion}

*discuss: directional selectivity and relationship to orientation prediction*

*discuss: DT fails because bands are not identical, results for Gabor, Gauss may seem surprising until we consider the analysis in our initial experiments in which we showed how the oriented band that produces maximum response is frequently not the band closest in direction to the structure when a realistic level noise is present in the images. Thus in a significant number of pixels we may be randomly permuting the feature dimensions*

\begin{itemize}
  \item Using confidence in orientation prediction

  \begin{itemize}
    \item Possible uses
    \item Figure?
  \end{itemize}

  \item Not mentioned centreline


\end{itemize}

\input{discussion.tex}


\clearpage
\section{Conclusions}
\input{conclusions.tex}


\section*{Acknowledgements}
We thank Nick Kingsbury for the \dtcwt{} Matlab toolbox. Mammograms were provided by the Nightingale Breast Centre, South Manchester University Hospitals Trust, UK and were annotated by Dr Caroline Boggis and Dr Rumana Rahim. This work was funded by EPSRC grant EP/E031307/1.

\section*{Additional tables}
%
\begin{table}[h]
\centering
\small
\input{experiments/tab_DRIVE_training_3.tex}
\caption{Detecting and predicting the orientation of retinal vessels. DRIVE database, training images:
effect of pooling responses from all scales}
\label{t:drive_training_3}
\end{table}
%
\begin{table*}[t]
\centering
\small
\input{experiments/tab_DRIVE_training_cost_sort2.tex}
\caption{Detecting and predicting the orientation of retinal vessels. DRIVE database, training images:
effect of pooling neighbourhood filter responses.}
\label{t:drive_training_c}
\end{table*}
%
\begin{table*}[t]
\centering
\small
\input{experiments/tab_DRIVE_training_cost_sort3.tex}
\caption{Detecting and predicting the orientation of retinal vessels. DRIVE database, training images:
effect of pooling neighbourhood filter responses.}
\label{t:drive_training_c}
\end{table*}
%
\begin{table*}[t]
\centering
\small
\input{experiments/tab_DRIVE_training_cost_sort4.tex}
\caption{Detecting and predicting the orientation of retinal vessels. DRIVE database, training images:
effect of pooling neighbourhood filter responses.}
\label{t:drive_training_c}
\end{table*}
%
\begin{table}[h]
\centering
\small
\input{experiments/tab_fibre_training_3.tex}
\caption{Detecting and predicting the orientation of nerve fibres in confocal corneal microscopy images. Training images:
effect of pooling responses from all scales.}
\label{t:fibre_training_3}
\end{table}
%
\begin{table*}[t]
\centering
\small
\input{experiments/tab_fibre_training_cost_sort.tex}
\caption{Detecting and predicting the orientation of retinal vessels. DRIVE database, training images:
effect of pooling neighbourhood filter responses.}
\label{t:fibre_training_c}
\end{table*}
%
\begin{table*}[t]
\centering
\small
\input{experiments/tab_fibre_training_cost_sort2.tex}
\caption{Detecting and predicting the orientation of retinal vessels. DRIVE database, training images:
effect of pooling neighbourhood filter responses.}
\label{t:fibre_training_c}
\end{table*}
%
\begin{table*}[t]
\centering
\small
\input{experiments/tab_fibre_training_cost_sort3.tex}
\caption{Detecting and predicting the orientation of retinal vessels. DRIVE database, training images:
effect of pooling neighbourhood filter responses.}
\label{t:fibre_training_c}
\end{table*}
%
\bibliographystyle{plain}
\bibliography{%
./bib/_aliases,%
./bib/mobio,%
./bib/mammography,%
./bib/ml,%
./bib/nailfold,%
./bib/papers_by_year,%
./bib/local}

\end{document} 